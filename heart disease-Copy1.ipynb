{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314b0bd1",
   "metadata": {},
   "source": [
    "# Cars Sales Prediction - ANN\n",
    "### 21131A1223 IML-Assigment-2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3e566",
   "metadata": {},
   "source": [
    "## 1. <u>Data Collection </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699c37c",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries...\n",
    "    1. numpy for array functions\n",
    "    2. pandas for reading the data\n",
    "    3. sklearn for model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371e353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703785e",
   "metadata": {},
   "source": [
    "### Reading the data from the csv file using pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14f3628",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc5 in position 5812: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./archive/car_purchasing.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:639\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc5 in position 5812: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./archive/car_purchasing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dbf6d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 2. <u> Data Visualising</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da0274",
   "metadata": {},
   "source": [
    "### Checking the data briefly...\n",
    "\n",
    "    - The dataset has twenty one (16) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb28aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580977e4",
   "metadata": {},
   "source": [
    "- The dataset has 3168 data samples\n",
    "- TenYearCHD is to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7232c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of labels: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e63283",
   "metadata": {},
   "source": [
    "    The columns education, cigsPerDay, BPMeds, totChol, BMI, heartRate, and glucose \n",
    "    have inconsistent null values. \n",
    "    Those values can be normalised in the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa42415",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 3.  <u> Data Pre-Processing </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381f52",
   "metadata": {},
   "source": [
    "### Filling the columns with null values with their respective mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe89278",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.education = data.education.fillna(data.education.mean())\n",
    "data.BPMeds = data.BPMeds.fillna(data.BPMeds.mean())\n",
    "data.cigsPerDay = data.cigsPerDay.fillna(data.cigsPerDay.mean())\n",
    "data.totChol = data.totChol.fillna(data.totChol.mean())\n",
    "data.BMI = data.BMI.fillna(data.BMI.mean())\n",
    "data.heartRate = data.heartRate.fillna(data.heartRate.mean())\n",
    "data.glucose = data.glucose.fillna(data.glucose.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89d869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afde976",
   "metadata": {},
   "source": [
    "####  All the null and inconsistent values are eliminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3f170",
   "metadata": {},
   "source": [
    "### Seperating features and labels <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3b85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7d81f",
   "metadata": {},
   "source": [
    "### Splitting the dataset into two using sklearn: \n",
    "    1. Train Data - 70% of the original data\n",
    "    2. Test Data - remaining 30% of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cba68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781068d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b758c6",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 4. <u> Building Model </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4d82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ad92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "cn = confusion_matrix(y_test,y_pred)\n",
    "cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0e045",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 5. <u> Testing the model </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b7d76",
   "metadata": {},
   "source": [
    "### Testing the model against the previously taken aside 30% test data (which the model has'nt seen even once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fe3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c728b0",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 6.  <u> Analysing the model performance using confusion matrix </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a94783",
   "metadata": {},
   "source": [
    "    By analysing the above bar chart we can say that the difference between Actual and predicted values is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620cfe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0fd9b",
   "metadata": {},
   "source": [
    "    In the above Confusion matrix we can observe that the number of `true positives` and number of `true negatives` is significantly much higher than number of false positives and false negatives\n",
    "    - which is a good sign for a good classification model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c72500",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuray = accuracy_score(y_pred, y_test)\n",
    "f1 = f1_score(y_pred, y_test, average = \"weighted\")\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(\"Accuracy:\", accuray)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812daf2",
   "metadata": {},
   "source": [
    "    - The accuracy of the model is around 85 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c23858",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## ChatGPT 3.5 prompts:\n",
    "\n",
    "### prompt1: \n",
    "\n",
    "    Why using logistic regression can be beneficial than any other classification methods\n",
    "    What problem scenarios can be considered to solve effectively using logistic regression classification\n",
    "\n",
    "### response:\n",
    "\n",
    "    Logistic regression is a statistical method used for binary classification tasks, where the target variable has two possible outcomes. While there isn't a universal rule that logistic regression is always superior to other classification methods, it does have several benefits in certain scenarios:\n",
    "\n",
    "1. **Interpretability**: Logistic regression provides easy interpretability of the results. The coefficients obtained from logistic regression can be interpreted as the effect of each feature on the log-odds of the outcome. This makes it particularly useful when the interpretability of the model is important, such as in medical or social sciences.\n",
    "\n",
    "2. **Efficiency with Small Datasets**: Logistic regression can perform well with small datasets. It's less prone to overfitting compared to more complex models like decision trees or neural networks, especially when the number of features is relatively small.\n",
    "\n",
    "3. **Efficiency with Linearly Separable Data**: When the classes are linearly separable, logistic regression tends to perform well and converge quickly. Linear separability means that the two classes can be separated by a straight line (or hyperplane in higher dimensions).\n",
    "\n",
    "4. **No Assumptions About Feature Distribution**: Logistic regression does not require the features to have a normal distribution or to be scaled in a specific way. This makes it less sensitive to feature scaling compared to some other algorithms like Support Vector Machines.\n",
    "\n",
    "5. **Regularization**: Logistic regression can be easily regularized to prevent overfitting, by adding penalties like L1 or L2 regularization to the cost function. This helps in dealing with multicollinearity and reducing model complexity.\n",
    "\n",
    "6. **Probabilistic Output**: Logistic regression models output probabilities which can be directly interpreted as the probability of belonging to a particular class. This is particularly useful in scenarios where not only the class label is important, but also the confidence associated with the prediction.\n",
    "\n",
    "Logistic regression is particularly effective in scenarios such as:\n",
    "\n",
    "1. **Medical Diagnostics**: Predicting whether a patient has a particular disease based on various medical tests and patient characteristics.\n",
    "  \n",
    "2. **Customer Churn Prediction**: Determining whether a customer is likely to churn or continue using a service based on their past behavior and demographic information.\n",
    "\n",
    "3. **Credit Risk Analysis**: Assessing the risk associated with extending credit to a customer based on their credit history and financial information.\n",
    "\n",
    "4. **Employee Attrition Prediction**: Predicting whether an employee is likely to leave a company based on various factors such as job satisfaction, salary, and work-life balance.\n",
    "\n",
    "5. **Marketing Response Prediction**: Predicting whether a customer will respond positively to a marketing campaign based on their demographic information and past interactions with the company.\n",
    "\n",
    "In these scenarios, logistic regression can provide accurate predictions while maintaining interpretability and efficiency, making it a popular choice for classification tasks. However, it's essential to evaluate different algorithms and choose the one that best suits the specific problem and data characteristics.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### prompt2:  \n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.86      0.92      1255\n",
    "           1       0.06      0.65      0.11        17\n",
    "\n",
    "    accuracy                           0.86      1272\n",
    "   macro avg       0.53      0.75      0.51      1272\n",
    "weighted avg       0.98      0.86      0.91      1272\n",
    "\n",
    "Accuracy: 0.8561320754716981\n",
    "F1 Score: 0.9108765668754882\n",
    "\n",
    "the above are the stats of a logistic regression model performance what can be concluded and are there any better ways or approaches for improvement\n",
    "    \n",
    "    \n",
    "### response:\n",
    "        \n",
    "From the provided statistics, we can make several observations about the performance of the logistic regression model:\n",
    "\n",
    "1. **Accuracy**: The overall accuracy of the model is 85.6%, which indicates that it correctly classifies approximately 85.6% of the instances in the dataset. However, accuracy alone may not provide a complete picture of the model's performance, especially in imbalanced datasets.\n",
    "\n",
    "2. **Precision and Recall**: Precision measures the ratio of correctly predicted positive observations to the total predicted positives, while recall measures the ratio of correctly predicted positive observations to the all observations in actual class. In this case, the precision for class 1 (the minority class) is low (6%), indicating that when the model predicts the positive class, it is incorrect most of the time. However, the recall for class 1 is relatively higher (65%), indicating that the model is able to identify a significant portion of the actual positive instances, though it also misclassifies many negative instances as positive.\n",
    "\n",
    "3. **F1-score**: The F1-score is the harmonic mean of precision and recall and provides a balance between the two metrics. The weighted average F1-score in this case is 0.91, which is relatively high, but it is important to note that the F1-score may not be the most appropriate metric in imbalanced datasets.\n",
    "\n",
    "4. **Support**: Support refers to the number of actual occurrences of each class in the dataset. In this case, there are 1255 instances of class 0 and only 17 instances of class 1, indicating a significant class imbalance.\n",
    "\n",
    "Based on these observations, we can conclude that while the logistic regression model achieves high accuracy and F1-score, it performs poorly in terms of precision for the minority class. This imbalance in precision and recall indicates that the model may not be effectively capturing the characteristics of the minority class.\n",
    "\n",
    "To improve the performance of the logistic regression model or explore alternative approaches, several strategies can be considered:\n",
    "\n",
    "1. **Address Class Imbalance**: Since the dataset is imbalanced with a small number of instances in the minority class, techniques such as oversampling, undersampling, or generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be employed to balance the class distribution.\n",
    "\n",
    "2. **Feature Engineering**: Feature engineering techniques can be used to extract more informative features from the data, which may improve the model's ability to discriminate between classes.\n",
    "\n",
    "3. **Hyperparameter Tuning**: Fine-tuning hyperparameters of the logistic regression model, such as regularization strength, can help improve its performance.\n",
    "\n",
    "4. **Ensemble Methods**: Ensemble methods like Random Forest, Gradient Boosting, or stacking can be used to combine multiple classifiers to improve predictive performance.\n",
    "\n",
    "5. **Model Selection**: It's also worth exploring other classification algorithms such as Support Vector Machines, Random Forest, Gradient Boosting, or Neural Networks to see if they can achieve better performance compared to logistic regression.\n",
    "\n",
    "6. **Threshold Adjustment**: Adjusting the classification threshold may also help balance precision and recall, especially in imbalanced datasets.\n",
    "\n",
    "7. **Error Analysis**: Conducting a thorough error analysis to understand the types of errors made by the model can provide insights into areas for improvement.\n",
    "\n",
    "By implementing these strategies and experimenting with different approaches, it's possible to enhance the performance of the classification model beyond what logistic regression alone can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59817015",
   "metadata": {},
   "source": [
    "[GitHub Repo Link - mshreeram](https://github.com/mshreeram/IML-Assignment-2/blob/main/heart%20disease.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
